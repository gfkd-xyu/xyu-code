{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n",
    "from imgaug import augmenters as iaa\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "%matplotlib inline \n",
    "\n",
    "# Path to trained weights file\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "# Directory to save logs and model checkpoints, if not provided\n",
    "# through the command line argument --logs\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Results directory\n",
    "# Save submission files here\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"results/breast/\")\n",
    "\n",
    "# The dataset doesn't have a standard train/val split, so I picked\n",
    "# a variety of images to surve as a validation set.\n",
    "# VAL_IMAGE_IDS = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastConfig(Config):\n",
    "    \"\"\"Configuration for training on the breast segmentation dataset.\"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"breast\"\n",
    "\n",
    "    # Adjust depending on your GPU memory\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # Background + nucleus\n",
    "\n",
    "    # Number of training and validation steps per epoch\n",
    "    STEPS_PER_EPOCH = (1231 - 369) // IMAGES_PER_GPU\n",
    "    VALIDATION_STEPS = max(1, 369 // IMAGES_PER_GPU)\n",
    "\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between nucleus and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "\n",
    "    # Backbone network architecture\n",
    "    # Supported values are: resnet50, resnet101\n",
    "    BACKBONE = \"resnet101\"\n",
    "\n",
    "    # Input image resizing\n",
    "    # Random crops of size 512x512\n",
    "    IMAGE_RESIZE_MODE = \"square\"\n",
    "    IMAGE_MIN_DIM = 512\n",
    "    IMAGE_MAX_DIM = 512\n",
    "    IMAGE_CHANNEL_COUNT = 3\n",
    "    IMAGE_MIN_SCALE = 0\n",
    "\n",
    "    # Length of square anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)\n",
    "\n",
    "    # ROIs kept after non-maximum supression (training and inference)\n",
    "    POST_NMS_ROIS_TRAINING = 1000 \n",
    "    POST_NMS_ROIS_INFERENCE = 2000\n",
    "\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    # RPN_NMS_THRESHOLD = 0.9\n",
    "\n",
    "    # How many anchors per image to use for RPN training\n",
    "    RPN_TRAIN_ANCHORS_PER_IMAGE = 64\n",
    "    \n",
    "    # Grayscale images\n",
    "    # IMAGE_CHANNEL_COUNT = 1\n",
    "    # Image mean (Grayscale)\n",
    "    MEAN_PIXEL = np.array([32768.0,32786.0,32768.0])\n",
    "\n",
    "    # If enabled, resizes instance masks to a smaller size to reduce\n",
    "    # memory load. Recommended when using high-resolution images.\n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\n",
    "\n",
    "    # Number of ROIs per image to feed to classifier/mask heads\n",
    "    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\n",
    "    # enough positive proposals to fill this and keep a positive:negative\n",
    "    # ratio of 1:3. You can increase the number of proposals by adjusting\n",
    "    # the RPN NMS threshold.\n",
    "    TRAIN_ROIS_PER_IMAGE = 128\n",
    "\n",
    "    # Maximum number of ground truth instances to use in one image\n",
    "    MAX_GT_INSTANCES = 200\n",
    "\n",
    "    # Max number of final detections per image\n",
    "    DETECTION_MAX_INSTANCES = 10\n",
    "\n",
    "\n",
    "class BreastInferenceConfig(BreastConfig):\n",
    "    # Set batch size to 1 to run one image at a time\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    # Don't resize imager for inferencing\n",
    "    #IMAGE_RESIZE_MODE = \"pad64\"\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.7\n",
    "    DETECTION_MIN_CONFIDENCE = 0.5\n",
    "\n",
    "############################################################\n",
    "#  Dataset\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreastDataset(utils.Dataset):\n",
    "\n",
    "    def load_breast(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the breast dataset.\n",
    "\n",
    "        dataset_dir: Root directory of the dataset\n",
    "        subset: Subset to load. Either the name of the sub-directory,\n",
    "                such as stage1_train, stage1_test, ...etc. or, one of:\n",
    "                * train: stage1_train excluding validation images\n",
    "                * val: validation images from VAL_IMAGE_IDS\n",
    "        \"\"\"\n",
    "        # Add classes. We have one class.\n",
    "        # Naming the dataset nucleus, and the class nucleus\n",
    "        self.add_class(\"breast\", 1, \"breast\")\n",
    "        df = pd.read_csv(CSV_DIR)\n",
    "        # Which subset?\n",
    "        # \"val\": use hard-coded list above\n",
    "        # \"train\": use data from stage1_train minus the hard-coded list above\n",
    "        # else: use the data from the specified sub-directory\n",
    "        assert subset in [\"train\", \"val\", \"test\"]\n",
    "        subset_dir = \"train\" if subset in [\"train\", \"val\"] else subset\n",
    "        dataset_dir = os.path.join(dataset_dir, subset_dir)\n",
    "        if subset == \"test\":\n",
    "            image_ids = []\n",
    "            for d in os.listdir(dataset_dir):\n",
    "                _,p, p_id, rl, iv = d.split(\"_\")\n",
    "                for f in os.listdir(os.path.join(dataset_dir,d,\"masks\")):\n",
    "                    # x = df[(df['patient_id']==\"P_\"+p_id)&(df['left or right breast']==rl)&\n",
    "                    #        (df['image view']==iv)&(df['abnormality id']==int(f[0]))]['pathology'].values[0]\n",
    "                    bd = df[(df['patient_id']==\"P_\"+p_id)&(df['left or right breast']==rl)&\n",
    "                            (df['image view']==iv)&(df['abnormality id']==int(f[0]))]['breast_density'].values[0]\n",
    "                    if bd==2 or bd==1:\n",
    "                        image_ids.append(d)\n",
    "            #image_ids = os.listdir(dataset_dir)\n",
    "        else:\n",
    "            x=[]\n",
    "            for d in os.listdir(dataset_dir):\n",
    "                _, p, p_id, rl, iv=d.split(\"_\")\n",
    "                for f in os.listdir(os.path.join(dataset_dir,d,\"masks\")):\n",
    "                   # x = df[(df['patient_id']==\"P_\"+p_id)&(df['left or right breast']==rl)&\n",
    "                    #        (df['image view']==iv)&(df['abnormality id']==int(f[0]))]['pathology'].values[0]\n",
    "                    bd = df[(df['patient_id']==\"P_\"+p_id)&(df['left or right breast']==rl)&\n",
    "                            (df['image view']==iv)&(df['abnormality id']==int(f[0]))]['breast_density'].values[0]\n",
    "                    if bd==2 or bd==1:\n",
    "                        x.append(d)\n",
    "            #x = os.listdir(dataset_dir)\n",
    "            y = np.ones(len(x))\n",
    "            train_x, val_x, _, _, = train_test_split(x, y, test_size=0.3, random_state=7)\n",
    "            if subset == \"val\":\n",
    "                image_ids = val_x\n",
    "            else:\n",
    "                # Get image ids from directory names\n",
    "                image_ids = train_x\n",
    "\n",
    "        # Add images\n",
    "        for image_id in image_ids:\n",
    "            self.add_image(\n",
    "                \"breast\",\n",
    "                image_id=image_id,\n",
    "                path=os.path.join(dataset_dir, image_id, \"images/000000.png\"))\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        # Get mask directory from image path\n",
    "        mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"masks\")\n",
    "\n",
    "        # Read mask files from .png image\n",
    "        mask = []\n",
    "        for f in os.listdir(mask_dir):\n",
    "            if f.endswith(\".png\"):\n",
    "                #ds = sitk.ReadImage(os.path.join(mask_dir, f))\n",
    "                #m = sitk.GetArrayFromImage(ds)\n",
    "                #m = np.squeeze(m)\n",
    "                #m = m.astype(np.bool)\n",
    "                m = skimage.io.imread(os.path.join(mask_dir, f)).astype(np.bool)\n",
    "                mask.append(m)\n",
    "        mask = np.stack(mask, axis=-1)\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID, we return an array of ones\n",
    "        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"breast\":\n",
    "            return info[\"id\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /backup/yuxin/anaconda3/envs/maskrcnn/lib/python3.6/site-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "Loading weights from  /backup/yuxin/Mask_RCNN/logs/breast20190412T1216/mask_rcnn_breast_0068.h5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (addr overflow, addr = 800, size = 8336, eoa = 2048)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-107027842af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Load trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading weights from \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Mask_RCNN/mrcnn/model.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, exclude)\u001b[0m\n\u001b[1;32m   2116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskrcnn/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskrcnn/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (addr overflow, addr = 800, size = 8336, eoa = 2048)"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(BreastInferenceConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=DEFAULT_LOGS_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "#model_path = '/backup/yuxin/Mask_RCNN/logs/breast20190313T1258/mask_rcnn_breast_0100.h5'\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(BreastInferenceConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    IMAGE_RESIZE_MODE = \"none\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='/backup/yuxin/CBIS-MASS-PNG/'\n",
    "CSV_DIR = \"/backup/yuxin/mass_case_description_train_set.csv\"\n",
    "subset='train'#test\n",
    "\n",
    "breast_ori = BreastDataset()\n",
    "\n",
    "breast_ori.load_breast(dataset,subset)\n",
    "\n",
    "breast_ori.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 b: 212\n",
      "0 a: 303\n",
      "1 a: 382\n",
      "1 a: 419\n",
      "2 b: 87\n",
      "17 a: 458\n",
      "18 a: 655\n",
      "Mass-Training_P_00703_LEFT_CC\n",
      "Mass-Training_P_00765_RIGHT_CC\n",
      "Mass-Training_P_01423_RIGHT_CC\n",
      "Mass-Training_P_00108_LEFT_CC\n",
      "Mass-Training_P_00059_LEFT_MLO\n",
      "Mass-Training_P_00765_RIGHT_MLO\n",
      "71 a: 1365\n",
      "Mass-Training_P_01182_LEFT_MLO\n",
      "81 b: 84\n",
      "Mass-Training_P_02033_RIGHT_CC\n",
      "Mass-Training_P_02092_LEFT_MLO\n",
      "Mass-Training_P_00715_RIGHT_MLO\n",
      "Mass-Training_P_01686_RIGHT_CC\n",
      "Mass-Training_P_01048_RIGHT_CC\n",
      "Mass-Training_P_00694_RIGHT_MLO\n",
      "Mass-Training_P_01981_RIGHT_CC\n",
      "Mass-Training_P_00927_LEFT_MLO\n",
      "Mass-Training_P_02092_LEFT_CC\n",
      "Mass-Training_P_01908_LEFT_CC\n",
      "Mass-Training_P_01831_RIGHT_CC\n",
      "Mass-Training_P_01182_LEFT_CC\n",
      "Mass-Training_P_01115_RIGHT_CC\n",
      "Mass-Training_P_01048_RIGHT_MLO\n",
      "Mass-Training_P_00859_LEFT_CC\n",
      "Mass-Training_P_00453_LEFT_MLO\n",
      "Mass-Training_P_01946_RIGHT_MLO\n",
      "Mass-Training_P_01983_LEFT_MLO\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "b=2048\n",
    "for i in breast_ori.image_ids:\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(breast_ori, InferenceConfig, i, use_mini_mask=False)\n",
    "    mask, class_id = breast_ori.load_mask(i)\n",
    "    #print(gt_bbox.shape)\n",
    "    #print(gt_bbox[0],image.shape,mask.shape)\n",
    "    if image.shape[:2]!=mask.shape[:2]: print(breast_ori.image_info[i]['id'])\n",
    "    for l in range(gt_bbox.shape[0]):\n",
    "        if min(abs(gt_bbox[l][2]-gt_bbox[l][0]),abs(gt_bbox[l][3]-gt_bbox[l][1])) < b:\n",
    "            b=min(abs(gt_bbox[l][2]-gt_bbox[l][0]),abs(gt_bbox[l][3]-gt_bbox[l][1]))\n",
    "            print(i,\"b:\",b)\n",
    "        if max(abs(gt_bbox[l][2]-gt_bbox[l][0]),abs(gt_bbox[l][3]-gt_bbox[l][1])) > a:\n",
    "            a=max(abs(gt_bbox[l][2]-gt_bbox[l][0]),abs(gt_bbox[l][3]-gt_bbox[l][1]))\n",
    "            print(i,\"a:\",a)\n",
    "print(a,b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mass-Test_P_00813_RIGHT_MLO\n"
     ]
    }
   ],
   "source": [
    "print(breast_ori.image_info[218]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetdir='/backup/yuxin/CBIS-Mass-Patches1024-New/'\n",
    "\n",
    "CSV_DIR = \"/backup/yuxin/mass_case_description_test_set.csv\"\n",
    "\n",
    "subset='test'#test\n",
    "\n",
    "breast_test = BreastDataset()\n",
    "\n",
    "breast_test.load_breast(datasetdir,subset)\n",
    "\n",
    "breast_test.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_test.image_info[200]['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids =breast_test.image_ids\n",
    "print(len(image_ids))\n",
    "APs = []\n",
    "recallss = []\n",
    "overlapss = []\n",
    "\n",
    "for image_id in image_ids:\n",
    "   # try:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "            modellib.load_image_gt(breast_test, inference_config,\n",
    "                                   image_id, use_mini_mask=False)\n",
    "    #print(gt_class_id)\n",
    "    #molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    print(r['scores'])\n",
    "    #print(image_id,breast_test.image_info[image_id]['id'])\n",
    "    #print(image.shape,gt_mask.shape,r['masks'].shape)\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "            utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                             r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    #active_class_ids=parse_image_meta(image_meta)['active_class_ids']\n",
    "    #l_cls=modellib.mrcnn_class_loss_graph(gt_class_id, r['class_ids'], active_class_ids)\n",
    "    #l_bbx=modelib.mrcnn_bbox_loss_graph(gt_bbox,gt_class_id,r['rois'])\n",
    "    #l_mask=modelib.mrcnn_mask_loss_graph(gt_mask, gt_class_id, r['masks'])\n",
    "    APs.append(AP)\n",
    "   # df = pd.data\n",
    "   # except:\n",
    "    #    continue\n",
    "    print(AP)\n",
    "    #print(breast_test.image_info[image_id]['id'])\n",
    "    #recallss.append(recalls)\n",
    " #   overlapss.append(overlaps)\n",
    "#print(APs)\n",
    "print(\"mAP: \", np.mean(APs))\n",
    "\n",
    "#print(\"recall: \", recallss)\n",
    "#print(\"overlap: \", np.mean(overlapss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
