{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-dev20200307\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import DataLoader\n",
    "import model\n",
    "from model import MyModel1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot \n",
    "pydot.find_graphviz = lambda: True \n",
    "from tensorflow.keras.utils import plot_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/train\n",
      "/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/test\n",
      "/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/val\n"
     ]
    }
   ],
   "source": [
    "train_dir = \"/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/train\"\n",
    "val_dir = \"/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/val\"\n",
    "test_dir = \"/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/test\"\n",
    "csv_path = \"/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/df.csv\"\n",
    "\n",
    "dataset_train = DataLoader.Dataset()\n",
    "dataset_test = DataLoader.Dataset()\n",
    "dataset_val = DataLoader.Dataset()\n",
    "\n",
    "dataset_train.load_dataset(train_dir)\n",
    "dataset_test.load_dataset(test_dir)\n",
    "dataset_val.load_dataset(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_val = DataLoader.Dataset()\n",
    "#dataset_val.load_dataset(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = iaa.SomeOf((0, 2), [iaa.Fliplr(0.5),\n",
    "                                   iaa.Flipud(0.5),\n",
    "                                   iaa.OneOf([iaa.Affine(rotate=90),\n",
    "                                              iaa.Affine(rotate=180),\n",
    "                                              iaa.Affine(rotate=270)])])\n",
    "        #iaa.Multiply((0.8, 1.5)),\n",
    "        #iaa.Invert(1,per_channel=True,max_value=65535)\n",
    "       # iaa.GaussianBlur(sigma=(0.0, 5.0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_i=dataset_train.load_case(dataset_train.case_id[0], csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_i[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "print(np.max(im))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img_id = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset_train.case_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    #assert isinstance(dataset, Dataset), \"dadaset is not belong to Dataset class\"\n",
    "    for i in dataset_train.case_id:\n",
    "        img, class_id = dataset_train.load_case_image(i, IMG_ID)\n",
    "        det = augment.to_deterministic()\n",
    "        img = det.augment_image(img)\n",
    "        yield img, class_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator():\n",
    "    #assert isinstance(dataset, Dataset), \"dadaset is not belong to Dataset class\"\n",
    "    for i in dataset_test.case_id:\n",
    "        img, class_id = dataset_test.load_case_image(i, IMG_ID)[0]\n",
    "        yield img, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def val_generator(augment=False):\n",
    "    #assert isinstance(dataset, Dataset), \"dadaset is not belong to Dataset class\"\n",
    "    for i in dataset_val.image_id:\n",
    "        img, class_id = dataset_val.load_image(i)\n",
    "        yield img, class_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id=dataset_train.case_id[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  group  sex       age    score2  smallbin       bmi  time_onset  \\\n",
      "0    1      1  0.0  0.914894  0.586582       1.0  0.246592    0.009036   \n",
      "1    1      0  0.0  0.872340  0.720968       1.0  0.459098    0.132530   \n",
      "2    2      1  1.0  0.106383  0.128921       0.0  0.364911    0.024096   \n",
      "3    2      0  1.0  0.000000  0.153463       0.0  0.525194    0.018072   \n",
      "4    3      0  1.0  0.468085  0.511229       0.0  0.299540    0.493976   \n",
      "5    3      1  1.0  0.446809  0.532517       1.0  0.451776    0.060241   \n",
      "6    4      0  1.0  0.404255  0.232390       0.0  1.000000    0.003012   \n",
      "7    4      1  1.0  0.531915  0.232192       1.0  0.104733    0.060241   \n",
      "8    5      1  1.0  0.468085  0.804762       0.0  0.615205    0.060241   \n",
      "9    5      0  1.0  0.468085  0.800563       0.0  0.410244    0.012048   \n",
      "10   6      0  1.0  0.382979  0.149044       0.0  0.540006    0.000000   \n",
      "11   6      1  1.0  0.468085  0.144864       0.0  0.314584    0.060241   \n",
      "12   7      0  1.0  0.531915  0.193367       0.0  0.200924    0.349398   \n",
      "13   7      1  1.0  0.446809  0.179741       0.0  0.344528    0.048193   \n",
      "14   8      1  1.0  0.468085  0.220055       0.0  0.464895    0.060241   \n",
      "15   8      0  1.0  0.425532  0.197854       0.0  0.464895    0.060241   \n",
      "16   9      0  0.0  0.702128  0.532754       1.0  0.519299    0.132530   \n",
      "17   9      1  0.0  0.638298  0.582339       1.0  0.306468    0.012048   \n",
      "18  10      1  1.0  0.744681  0.076659       0.0  0.297872    0.000000   \n",
      "19  10      0  1.0  0.638298  0.060024       0.0  0.468085    0.000000   \n",
      "20  11      0  1.0  0.319149  0.073724       1.0  0.510638    0.060241   \n",
      "21  11      1  1.0  0.276596  0.067520       0.0  0.174109    0.060241   \n",
      "22  12      0  1.0  0.659574  0.392658       1.0  0.314584    0.012048   \n",
      "23  12      1  1.0  0.723404  0.619724       1.0  0.496123    0.018072   \n",
      "24  13      0  0.0  0.723404  0.340959       1.0  0.445676    0.277108   \n",
      "25  13      1  0.0  0.787234  0.395780       1.0  0.365373    0.042169   \n",
      "26  14      0  1.0  0.553191  0.165665       0.0  0.205838    0.042169   \n",
      "27  14      1  1.0  0.404255  0.164131       0.0  0.363675    0.060241   \n",
      "28  15      0  1.0  0.659574  0.270644       0.0  0.216840    0.060241   \n",
      "29  15      1  1.0  0.617021  0.268214       0.0  0.319625    0.277108   \n",
      "..  ..    ...  ...       ...       ...       ...       ...         ...   \n",
      "52  27      0  1.0  0.510638  0.423265       1.0  0.226950    0.277108   \n",
      "53  27      1  1.0  0.489362  1.000000       1.0  0.397163    0.060241   \n",
      "54  28      0  1.0  0.276596  0.437652       0.0  0.423552    0.060241   \n",
      "55  28      1  1.0  0.234043  0.429567       1.0  0.469070    0.003012   \n",
      "56  29      0  0.0  0.617021  0.034781       1.0  0.561390    0.277108   \n",
      "57  29      1  0.0  0.744681  0.194657       1.0  0.437679    0.027108   \n",
      "58  30      0  1.0  0.531915  0.253827       0.0  0.583928    0.349398   \n",
      "59  30      1  1.0  0.510638  0.251426       0.0  0.302314    0.021084   \n",
      "60  31      1  1.0  0.510638  0.271205       0.0  0.423552    0.024096   \n",
      "61  31      0  1.0  0.425532  0.323716       0.0  0.613181    0.060241   \n",
      "62  32      0  1.0  0.319149  0.485457       0.0  0.564890    0.030120   \n",
      "63  32      1  1.0  0.191489  0.569856       0.0  0.331815    0.060241   \n",
      "64  33      0  0.0  0.531915  0.096200       1.0  0.219760    0.204819   \n",
      "65  33      1  0.0  0.468085  0.182413       0.0  0.255546    0.033133   \n",
      "66  34      0  1.0  0.851064  0.264215       1.0  0.538877    0.349398   \n",
      "67  34      1  1.0  0.851064  0.515484       0.0  0.226950    0.132530   \n",
      "68  35      1  1.0  0.574468  0.015950       0.0  0.158106    0.204819   \n",
      "69  35      0  1.0  0.659574  0.040421       0.0  0.467095    0.132530   \n",
      "70  36      1  1.0  0.468085  0.234274       1.0  0.496123    0.060241   \n",
      "71  36      0  1.0  0.510638  0.383551       1.0  0.510510    0.710843   \n",
      "72  37      1  1.0  0.553191  0.231204       1.0  0.416741    0.060241   \n",
      "73  37      0  1.0  0.489362  0.217976       0.0  0.397163    0.060241   \n",
      "74  38      1  1.0  0.319149  0.091494       0.0  0.539973    0.012048   \n",
      "75  38      0  1.0  0.276596  0.148934       0.0  0.647606    0.042169   \n",
      "76  39      0  1.0  0.617021  0.127690       0.0  0.435507    0.493976   \n",
      "77  39      1  1.0  0.638298  0.342533       1.0  0.314584    0.132530   \n",
      "78  40      1  0.0  0.574468  0.000000       1.0  0.142233    0.060241   \n",
      "79  40      0  0.0  0.446809  0.035295       1.0  0.449864    0.060241   \n",
      "80  41      1  0.0  0.638298  0.152678       1.0  0.437621    0.024096   \n",
      "81  41      0  0.0  0.765957  0.340809       1.0  0.450216    0.132530   \n",
      "\n",
      "    ane_diam  diabetes  hs_cad  hyperlipidemia  HBP       SBP       DBP  ECG  \\\n",
      "0   0.666667       1.0     0.0             0.0  1.0  0.624060  0.855263  1.0   \n",
      "1   0.752000       1.0     0.0             1.0  1.0  0.368421  0.697368  1.0   \n",
      "2   0.626667       0.0     0.0             0.0  1.0  0.368421  0.697368  0.0   \n",
      "3   0.000000       0.0     0.0             0.0  1.0  0.548872  0.723684  0.0   \n",
      "4   0.000000       0.0     0.0             0.0  1.0  0.398496  0.592105  1.0   \n",
      "5   0.680000       0.0     0.0             0.0  1.0  0.466165  0.263158  1.0   \n",
      "6   0.000000       0.0     0.0             0.0  0.0  0.624060  0.723684  0.0   \n",
      "7   0.533333       0.0     0.0             0.0  1.0  0.503759  0.539474  0.0   \n",
      "8   0.664000       0.0     0.0             0.0  1.0  0.285714  0.328947  1.0   \n",
      "9   0.746667       0.0     0.0             0.0  1.0  0.406015  0.342105  0.0   \n",
      "10  0.586667       0.0     0.0             1.0  1.0  0.451128  0.328947  1.0   \n",
      "11  0.600000       0.0     0.0             0.0  1.0  0.819549  0.907895  1.0   \n",
      "12  0.800000       0.0     0.0             0.0  1.0  0.353383  0.421053  1.0   \n",
      "13  0.561333       0.0     0.0             0.0  1.0  0.548872  0.460526  1.0   \n",
      "14  0.885333       0.0     0.0             1.0  1.0  0.263158  0.592105  1.0   \n",
      "15  0.000000       0.0     0.0             1.0  1.0  0.263158  0.592105  1.0   \n",
      "16  0.000000       1.0     0.0             0.0  1.0  0.571429  0.934211  1.0   \n",
      "17  0.626667       0.0     0.0             0.0  0.0  0.000000  0.342105  1.0   \n",
      "18  0.617333       0.0     0.0             0.0  1.0  1.000000  1.000000  1.0   \n",
      "19  0.586667       0.0     0.0             1.0  1.0  0.398496  0.460526  1.0   \n",
      "20  0.000000       0.0     0.0             0.0  0.0  0.571429  0.710526  1.0   \n",
      "21  0.573333       0.0     0.0             0.0  1.0  0.533835  0.592105  1.0   \n",
      "22  0.613333       0.0     0.0             0.0  0.0  0.548872  0.723684  0.0   \n",
      "23  0.546667       0.0     0.0             0.0  0.0  0.736842  0.802632  1.0   \n",
      "24  0.000000       0.0     0.0             0.0  0.0  0.488722  0.710526  0.0   \n",
      "25  0.520000       0.0     0.0             0.0  1.0  0.278196  0.723684  1.0   \n",
      "26  0.800000       0.0     0.0             0.0  1.0  0.398496  0.723684  1.0   \n",
      "27  0.711467       0.0     0.0             1.0  1.0  0.827068  0.934211  1.0   \n",
      "28  0.746667       0.0     0.0             0.0  1.0  0.390977  0.592105  1.0   \n",
      "29  0.586667       0.0     0.0             0.0  1.0  0.488722  0.605263  1.0   \n",
      "..       ...       ...     ...             ...  ...       ...       ...  ...   \n",
      "52  0.660000       0.0     0.0             0.0  0.0  0.240602  0.473684  1.0   \n",
      "53  0.573333       0.0     0.0             0.0  1.0  0.556391  0.671053  1.0   \n",
      "54  0.533333       0.0     0.0             0.0  1.0  0.308271  0.486842  1.0   \n",
      "55  0.581333       0.0     0.0             0.0  1.0  0.127820  0.328947  1.0   \n",
      "56  0.605333       0.0     0.0             1.0  1.0  0.774436  0.763158  0.0   \n",
      "57  0.566667       0.0     0.0             0.0  1.0  0.225564  0.342105  0.0   \n",
      "58  0.546667       0.0     0.0             1.0  1.0  0.548872  0.697368  1.0   \n",
      "59  0.600000       0.0     0.0             0.0  1.0  0.436090  0.605263  0.0   \n",
      "60  0.000000       0.0     0.0             0.0  1.0  0.624060  0.723684  1.0   \n",
      "61  0.684000       0.0     0.0             1.0  1.0  0.684211  1.000000  1.0   \n",
      "62  0.000000       0.0     0.0             0.0  1.0  0.661654  0.842105  1.0   \n",
      "63  0.000000       0.0     0.0             0.0  1.0  0.421053  0.539474  1.0   \n",
      "64  0.730667       0.0     0.0             0.0  1.0  0.240602  0.657895  1.0   \n",
      "65  0.626667       0.0     0.0             0.0  1.0  0.360902  0.552632  1.0   \n",
      "66  0.558667       0.0     0.0             0.0  1.0  0.548872  0.789474  1.0   \n",
      "67  0.693333       0.0     0.0             0.0  1.0  0.413534  0.605263  1.0   \n",
      "68  0.000000       0.0     0.0             0.0  1.0  0.451128  0.802632  0.0   \n",
      "69  0.673333       0.0     0.0             0.0  1.0  0.729323  0.907895  0.0   \n",
      "70  0.000000       0.0     0.0             1.0  1.0  0.624060  0.592105  1.0   \n",
      "71  0.533333       0.0     0.0             1.0  1.0  0.345865  0.486842  1.0   \n",
      "72  0.586667       0.0     0.0             0.0  0.0  0.458647  0.513158  0.0   \n",
      "73  0.666667       0.0     0.0             0.0  0.0  0.323308  0.592105  1.0   \n",
      "74  0.853333       0.0     0.0             0.0  1.0  0.323308  0.592105  1.0   \n",
      "75  0.521333       0.0     1.0             0.0  1.0  0.759399  0.000000  1.0   \n",
      "76  0.626667       0.0     0.0             0.0  1.0  0.458647  0.592105  1.0   \n",
      "77  0.640000       0.0     0.0             0.0  0.0  0.436090  0.671053  1.0   \n",
      "78  0.000000       0.0     0.0             0.0  1.0  0.473684  0.657895  0.0   \n",
      "79  0.908000       0.0     1.0             0.0  1.0  0.571429  0.855263  1.0   \n",
      "80  0.000000       0.0     0.0             0.0  0.0  0.473684  0.723684  1.0   \n",
      "81  0.586667       1.0     0.0             0.0  1.0  0.488722  0.776316  1.0   \n",
      "\n",
      "    kidney_failure  ln_ddimer  limb_ischemia  tamponade_presurgbin  \n",
      "0              0.0   0.847246            0.0                   0.0  \n",
      "1              0.0   0.426389            0.0                   1.0  \n",
      "2              0.0   0.669479            0.0                   0.0  \n",
      "3              0.0   0.421741            0.0                   1.0  \n",
      "4              0.0   1.000000            0.0                   0.0  \n",
      "5              0.0   0.651064            1.0                   0.0  \n",
      "6              0.0   0.805514            0.0                   0.0  \n",
      "7              0.0   0.646203            0.0                   0.0  \n",
      "8              0.0   0.997573            0.0                   1.0  \n",
      "9              0.0   1.000000            1.0                   1.0  \n",
      "10             0.0   0.487064            0.0                   0.0  \n",
      "11             0.0   0.265573            0.0                   0.0  \n",
      "12             0.0   0.276569            0.0                   0.0  \n",
      "13             0.0   0.401288            0.0                   0.0  \n",
      "14             0.0   0.454797            0.0                   0.0  \n",
      "15             0.0   0.444577            0.0                   0.0  \n",
      "16             0.0   0.391629            0.0                   1.0  \n",
      "17             0.0   0.412432            0.0                   1.0  \n",
      "18             0.0   0.000000            0.0                   0.0  \n",
      "19             0.0   0.071753            0.0                   0.0  \n",
      "20             0.0   0.264851            0.0                   0.0  \n",
      "21             0.0   0.261745            0.0                   0.0  \n",
      "22             0.0   0.890379            0.0                   0.0  \n",
      "23             0.0   0.461134            0.0                   1.0  \n",
      "24             0.0   0.219296            0.0                   1.0  \n",
      "25             0.0   0.457584            0.0                   0.0  \n",
      "26             0.0   0.315563            0.0                   0.0  \n",
      "27             0.0   0.399646            0.0                   0.0  \n",
      "28             0.0   0.424188            0.0                   0.0  \n",
      "29             0.0   0.469014            0.0                   0.0  \n",
      "..             ...        ...            ...                   ...  \n",
      "52             0.0   0.243274            0.0                   1.0  \n",
      "53             0.0   0.950695            1.0                   1.0  \n",
      "54             0.0   1.000000            0.0                   0.0  \n",
      "55             0.0   0.469947            0.0                   1.0  \n",
      "56             0.0   0.257642            0.0                   0.0  \n",
      "57             0.0   0.436856            0.0                   0.0  \n",
      "58             0.0   0.419112            0.0                   0.0  \n",
      "59             0.0   0.695326            0.0                   0.0  \n",
      "60             0.0   0.435389            0.0                   0.0  \n",
      "61             0.0   0.275552            1.0                   0.0  \n",
      "62             1.0   0.360961            0.0                   0.0  \n",
      "63             0.0   0.914786            1.0                   0.0  \n",
      "64             0.0   0.144028            0.0                   0.0  \n",
      "65             0.0   0.426600            0.0                   0.0  \n",
      "66             0.0   0.282549            0.0                   0.0  \n",
      "67             0.0   0.600323            0.0                   0.0  \n",
      "68             0.0   0.226463            0.0                   0.0  \n",
      "69             0.0   0.229827            0.0                   0.0  \n",
      "70             0.0   0.410121            0.0                   0.0  \n",
      "71             0.0   0.418743            1.0                   0.0  \n",
      "72             0.0   0.632627            0.0                   0.0  \n",
      "73             0.0   0.402913            0.0                   0.0  \n",
      "74             0.0   0.204766            0.0                   0.0  \n",
      "75             0.0   0.406119            0.0                   0.0  \n",
      "76             0.0   0.138065            0.0                   0.0  \n",
      "77             0.0   0.186619            1.0                   0.0  \n",
      "78             0.0   0.193343            0.0                   0.0  \n",
      "79             0.0   0.096656            0.0                   0.0  \n",
      "80             0.0   0.274014            0.0                   0.0  \n",
      "81             0.0   0.479316            0.0                   0.0  \n",
      "\n",
      "[82 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaData/20191108_41对/df.csv\", header=0)\n",
    "\n",
    "df.drop([\"tchol\",'hdl','ldl','trig','hypotension'], axis=1, inplace=True)\n",
    "df['ane_diam'].fillna(0,inplace=True)\n",
    "df.fillna(method='ffill',inplace=True)\n",
    "small = LabelBinarizer().fit(df[\"smallbin\"])\n",
    "df['smallbin']=small.transform(df['smallbin'])\n",
    "dia = LabelBinarizer().fit(df[\"diabetes\"])\n",
    "hbp = LabelBinarizer().fit(df[\"HBP\"])\n",
    "df['diabetes']=dia.transform(df['diabetes'])\n",
    "df['HBP']=hbp.transform(df['HBP'])\n",
    "cs = MinMaxScaler()\n",
    "df[df.columns[2:]] = cs.fit_transform(df[df.columns[2:]])\n",
    "df[df.columns[2:]] = df[df.columns[2:]].astype(np.float32)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sex       age    score2  smallbin       bmi  time_onset  ane_diam  \\\n",
      "39  1.0  0.340426  0.195975       0.0  0.363675    0.060241       0.0   \n",
      "\n",
      "    diabetes  hs_cad  hyperlipidemia  HBP       SBP       DBP  ECG  \\\n",
      "39       0.0     0.0             0.0  1.0  0.285714  0.381579  1.0   \n",
      "\n",
      "    kidney_failure  ln_ddimer  limb_ischemia  tamponade_presurgbin  \n",
      "39             0.0   0.431736            0.0                   0.0  \n",
      "[[1.         0.34042552 0.19597453 0.         0.36367512 0.06024097\n",
      "  0.         0.         0.         0.         1.         0.2857143\n",
      "  0.38157895 1.         0.         0.43173632 0.         0.        ]]\n",
      "(1, 18)\n"
     ]
    }
   ],
   "source": [
    "cid, group = case_id.split(\"_\")\n",
    "attribute = df[df.columns[2:]][(df['id']==int(cid))&(df['group']==1)] if group=='event' else df[(df['id']==int(cid))&(df['group']==0)]\n",
    "print(attribute)\n",
    "print(np.array(attribute))\n",
    "print(np.array(attribute).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_dataset = tf.data.Dataset.from_generator(\n",
    "    val_generator, (tf.float32, tf.int8), (tf.TensorShape([512, 512, 3]),tf.TensorShape([])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_train.case_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#train_dataset = train_dataset.make_one_shot_iterator()\n",
    "#_, labels = train_dataset.get_next()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 512, 3)\n",
      "(None, 512, 512)\n",
      "Epoch 1/20\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512)\n",
      "     21/Unknown - 11s 518ms/step - loss: 0.8593 - accuracy: 0.6500"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-451528251a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                   metrics=['accuracy'])\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#validation_data=test_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    IMG_ID = i+1\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        train_generator, (tf.float32, tf.int8), \n",
    "        (tf.TensorShape([Config.IMAGE_DIM, Config.IMAGE_DIM, 3]), tf.TensorShape([])))\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=len(dataset_train.case_id))\n",
    "    train_dataset = train_dataset.batch(1)\n",
    "\n",
    "    model = MyModel1(2)\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(train_dataset, epochs=20)#validation_data=test_dataset\n",
    "    model.summary()\n",
    "    \n",
    "    model.save_weights('/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights/{}/'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "ValueError: too many values to unpack (expected 2)\nTraceback (most recent call last):\n\n  File \"/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-17-c663bd52e4c1>\", line 4, in test_generator\n    img, class_id = dataset_test.load_case_image(i, IMG_ID)[0]\n\nValueError: too many values to unpack (expected 2)\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-d49055d97535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   metrics=['accuracy'])\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     return self._model_iteration(\n\u001b[1;32m    455\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2576\u001b[0m     \u001b[0;31m# tensors from the iterator and then standardize them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2578\u001b[0;31m       \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_tensors_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2579\u001b[0m     \u001b[0;31m# We type-check that `inputs` and `targets` are either single arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m     \u001b[0;31m# or lists of arrays, and extract a flat list of inputs from the passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mextract_tensors_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m   1580\u001b[0m   \"\"\"\n\u001b[1;32m   1581\u001b[0m   \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m   \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack_iterator_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36munpack_iterator_input\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \"\"\"\n\u001b[1;32m   1595\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1596\u001b[0;31m     \u001b[0mnext_element\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1597\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m     raise RuntimeError('Your dataset iterator ran out of data; '\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mget_next\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    735\u001b[0m     \"\"\"\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_gather_saveables_for_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2671\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2672\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2673\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2674\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2675\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: ValueError: too many values to unpack (expected 2)\nTraceback (most recent call last):\n\n  File \"/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/script_ops.py\", line 221, in __call__\n    ret = func(*args)\n\n  File \"/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\", line 585, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"<ipython-input-17-c663bd52e4c1>\", line 4, in test_generator\n    img, class_id = dataset_test.load_case_image(i, IMG_ID)[0]\n\nValueError: too many values to unpack (expected 2)\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    IMG_ID = i+1\n",
    "    test_dataset = tf.data.Dataset.from_generator(test_generator, \n",
    "            (tf.float32, tf.int8), (tf.TensorShape([Config.IMAGE_DIM, Config.IMAGE_DIM, 3]), tf.TensorShape([])))\n",
    "    test_dataset = test_dataset.shuffle(buffer_size=len(dataset_test.case_id))\n",
    "    test_dataset = test_dataset.batch(1)\n",
    "    model = MyModel1(2)\n",
    "    model.load_weights('/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights/{}/'.format(i+1))\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.evaluate(test_dataset)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = MyModel1(2)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(train_dataset, epochs=10)\n",
    "model.summary()\n",
    "\n",
    "model.save_weights('/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    504/Unknown - 180s 358ms/step - loss: 0.7053 - accuracy: 0.5179"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/pydicom/pixel_data_handlers/numpy_handler.py:267: UserWarning: The length of the pixel data in the dataset (524544 bytes) indicates it contains excess padding. 256 bytes will be removed from the end of the data\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 255s 505ms/step - loss: 0.7053 - accuracy: 0.5179 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "504/504 [==============================] - 244s 485ms/step - loss: 0.6929 - accuracy: 0.5238 - val_loss: 2.7229 - val_accuracy: 0.5000\n",
      "Epoch 3/5\n",
      "504/504 [==============================] - 242s 480ms/step - loss: 0.6937 - accuracy: 0.5179 - val_loss: 2.7322 - val_accuracy: 0.5000\n",
      "Epoch 4/5\n",
      "504/504 [==============================] - 245s 487ms/step - loss: 0.6909 - accuracy: 0.5099 - val_loss: 3.2879 - val_accuracy: 0.5000\n",
      "Epoch 5/5\n",
      "504/504 [==============================] - 244s 485ms/step - loss: 0.6899 - accuracy: 0.5337 - val_loss: 3.4100 - val_accuracy: 0.5000\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 16, 16, 2048)      23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#inputs = tf.keras.Input(shape=([None,None,14]))\n",
    "#x = tf.keras.layers.Conv2D(3,(3,3),padding=\"same\")(inputs)\n",
    "model_res = tf.keras.applications.ResNet50(include_top=False, weights='imagenet',input_shape=(512,512,3))\n",
    "model_res.trainable = False\n",
    "model = tf.keras.Sequential(\n",
    "[model_res,\n",
    "tf.keras.layers.GlobalAveragePooling2D(),\n",
    "tf.keras.layers.Dense(2, activation='sigmoid')])\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "model.summary()\n",
    "\n",
    "model.save_weights('/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights/')\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen():\n",
    "    #assert isinstance(dataset, Dataset), \"dadaset is not belong to Dataset class\"\n",
    "    for i in dataset_train.case_id:\n",
    "        attr, case, class_id = dataset_train.load_case(i, csv_path, augmentation=augment)\n",
    "        #inputs = [attr, case]\n",
    "        yield {\"input_1\": case,\"input_2\": attr}, class_id\n",
    "\n",
    "def val_gen():\n",
    "    #assert isinstance(dataset, Dataset), \"dadaset is not belong to Dataset class\"\n",
    "    for i in dataset_val.case_id:\n",
    "        attr, case, class_id = dataset_val.load_case(i, csv_path)\n",
    "        yield {\"input_1\": case,\"input_2\": attr}, class_id\n",
    "\n",
    "def test_gen():\n",
    "    #assert isinstance(dataset, Dataset), \"dadaset is not belong to Dataset class\"\n",
    "    for i in dataset_test.case_id:\n",
    "        attr, case, class_id = dataset_test.load_case(i, csv_path)\n",
    "        yield {\"input_1\": case,\"input_2\": attr}, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_d = tf.data.Dataset.from_generator(\n",
    "    train_gen, output_types=({\"input_1\": tf.float32,\"input_2\": tf.float32}, tf.int8), output_shapes=({\"input_1\": tf.TensorShape([14, Config.IMAGE_DIM, Config.IMAGE_DIM, 3]),\"input_2\": tf.TensorShape(18,)}, tf.TensorShape([])))\n",
    "\n",
    "val_d = tf.data.Dataset.from_generator(\n",
    "    val_gen, output_types=({\"input_1\": tf.float32,\"input_2\": tf.float32}, tf.int8), output_shapes=({\"input_1\": tf.TensorShape([14, Config.IMAGE_DIM, Config.IMAGE_DIM, 3]),\"input_2\": tf.TensorShape(18,)}, tf.TensorShape([])))\n",
    "\n",
    "test_d = tf.data.Dataset.from_generator(\n",
    "    test_gen, output_types=({\"input_1\": tf.float32,\"input_2\": tf.float32}, tf.int8), output_shapes=({\"input_1\": tf.TensorShape([14, Config.IMAGE_DIM, Config.IMAGE_DIM, 3]),\"input_2\": tf.TensorShape(18,)}, tf.TensorShape([])))\n",
    "\n",
    "train_d = train_d.shuffle(buffer_size=len(dataset_train.case_id))\n",
    "train_d = train_d.batch(1)\n",
    "\n",
    "val_d = val_d.shuffle(buffer_size=len(dataset_val.case_id))\n",
    "val_d = val_d.batch(1)\n",
    "\n",
    "test_d = test_d.shuffle(buffer_size=len(dataset_test.case_id))\n",
    "test_d = test_d.batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.datetime.now()\n",
    "model_dir = '/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights'\n",
    "log_dir = os.path.join(model_dir, \"{:%Y%m%dT%H%M}\".format(now))\n",
    "\n",
    "        # Path to save after each epoch. Include placeholders that get filled by Keras.\n",
    "checkpoint_path = os.path.join(log_dir, \"aorta_*epoch*.h5\")\n",
    "checkpoint_path = checkpoint_path.replace(\"*epoch*\", \"{epoch:04d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir,\n",
    "                                        histogram_freq=0, write_graph=True, write_images=False),\n",
    "    tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                            verbose=0, save_weights_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 512, 512, 3)\n",
      "(None, 1418)\n"
     ]
    }
   ],
   "source": [
    "#inputs = tf.keras.Input(shape=([512,512,14]))\n",
    "\n",
    "model = model.Aorta_Model(img_shape=([14, Config.IMAGE_DIM, Config.IMAGE_DIM, 3]),attr_shape=(18,), classes=2)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#pre_model = tf.keras.applications.Xception(include_top=False, input_shape=(512,512,3), weights='imagenet')\n",
    "#print(type(pre_model.layers))\n",
    "#print(dir(pre_model.layers))\n",
    "#for i in range(7):\n",
    " #   pre_model._layers.pop(0)\n",
    "#for layer in pre_model.layers:\n",
    "#    print(layer.name)\n",
    "#print(pre_model.get_layer('block1_conv1').name)    \n",
    "    \n",
    "plot_model(model, show_shapes=True, to_file='/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/{}.jpg'.format('xception')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "      7/Unknown - 289s 41s/step - loss: 0.7454 - accuracy: 0.6667"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-75da18c3e140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model.save_weights('/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights/depthNET.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_d, epochs=100, callbacks=callbacks_list, validation_data=val_d)\n",
    "\n",
    "\n",
    "\n",
    "#model.save_weights('/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights/depthNET.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100\n",
      "     43/Unknown - 32s 751ms/step - loss: 0.5855 - accuracy: 0.7857"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6195f77a8c2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.trainable=True\n",
    "model.fit(train_d, epochs=100, initial_epoch=30, callbacks=callbacks_list, validation_data=val_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 3s 203ms/step - loss: 0.6987 - accuracy: 0.5714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.698688805103302, 0.5714286]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
    "#model.load_weights('/Users/nikki/Documents/xyu_iterms/aorta_classification/aortaPy/weights/my_model.h5')\n",
    "#model.compile(optimizer=optimizer,\n",
    "#             loss='sparse_categorical_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "model.evaluate(test_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_case_attributes(csv_path):\n",
    "        df = pd.read_csv(csv_path,header=0)\n",
    "        df.drop([\"tchol\",'hdl','ldl','trig','hypotension','score2'], axis=1, inplace=True)\n",
    "        df['ane_diam'].fillna(0,inplace=True)\n",
    "        df.fillna(method='ffill',inplace=True)\n",
    "        small = LabelBinarizer().fit(df[\"smallbin\"])\n",
    "        df['smallbin']=small.transform(df['smallbin'])\n",
    "        dia = LabelBinarizer().fit(df[\"diabetes\"])\n",
    "        hbp = LabelBinarizer().fit(df[\"HBP\"])\n",
    "        df['diabetes']=dia.transform(df['diabetes'])\n",
    "        df['HBP']=hbp.transform(df['HBP'])\n",
    "        cs = MinMaxScaler()\n",
    "        df[df.columns[2:]] = cs.fit_transform(df[df.columns[2:]])\n",
    "        df[df.columns[2:]] = df[df.columns[2:]].astype(np.float32)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>group</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>smallbin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>time_onset</th>\n",
       "      <th>ane_diam</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>hs_cad</th>\n",
       "      <th>hyperlipidemia</th>\n",
       "      <th>HBP</th>\n",
       "      <th>SBP</th>\n",
       "      <th>DBP</th>\n",
       "      <th>ECG</th>\n",
       "      <th>kidney_failure</th>\n",
       "      <th>ln_ddimer</th>\n",
       "      <th>limb_ischemia</th>\n",
       "      <th>tamponade_presurgbin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.246592</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.459098</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.752000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.426389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.106383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.364911</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.669479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.525194</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299540</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.398496</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451776</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466165</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.651064</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.805514</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.104733</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503759</td>\n",
       "      <td>0.539474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.646203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.615205</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.997573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410244</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451128</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.487064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.314584</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.819549</td>\n",
       "      <td>0.907895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.265573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200924</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353383</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276569</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344528</td>\n",
       "      <td>0.048193</td>\n",
       "      <td>0.561333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.401288</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464895</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.885333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464895</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.444577</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.519299</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.391629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.306468</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.398496</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.264851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.174109</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.533835</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.261745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314584</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.613333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.890379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496123</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.461134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.445676</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.488722</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.219296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.365373</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.278196</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.457584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205838</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.398496</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.315563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.363675</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.711467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.827068</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216840</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.746667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.390977</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.424188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.319625</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.488722</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.243274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.397163</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.556391</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.423552</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308271</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.469070</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.581333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.127820</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469947</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.561390</td>\n",
       "      <td>0.277108</td>\n",
       "      <td>0.605333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.774436</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257642</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437679</td>\n",
       "      <td>0.027108</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.225564</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.583928</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.419112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302314</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.436090</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.423552</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.613181</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.275552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564890</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.661654</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.331815</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.539474</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.914786</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219760</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>0.730667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.240602</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.144028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.255546</td>\n",
       "      <td>0.033133</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.360902</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.538877</td>\n",
       "      <td>0.349398</td>\n",
       "      <td>0.558667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.548872</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.282549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226950</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.413534</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.600323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.158106</td>\n",
       "      <td>0.204819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.451128</td>\n",
       "      <td>0.802632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.226463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467095</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729323</td>\n",
       "      <td>0.907895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.229827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.468085</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496123</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.624060</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.510510</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.345865</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.418743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.553191</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416741</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.513158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.632627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.489362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397163</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.323308</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.539973</td>\n",
       "      <td>0.012048</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.323308</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.647606</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>0.521333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.759399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.406119</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.435507</td>\n",
       "      <td>0.493976</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.458647</td>\n",
       "      <td>0.592105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.138065</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.314584</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436090</td>\n",
       "      <td>0.671053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.186619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.574468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142233</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.449864</td>\n",
       "      <td>0.060241</td>\n",
       "      <td>0.908000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.437621</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.723684</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.450216</td>\n",
       "      <td>0.132530</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.488722</td>\n",
       "      <td>0.776316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  group  sex       age  smallbin       bmi  time_onset  ane_diam  \\\n",
       "0    1      1  0.0  0.914894       1.0  0.246592    0.009036  0.666667   \n",
       "1    1      0  0.0  0.872340       1.0  0.459098    0.132530  0.752000   \n",
       "2    2      1  1.0  0.106383       0.0  0.364911    0.024096  0.626667   \n",
       "3    2      0  1.0  0.000000       0.0  0.525194    0.018072  0.000000   \n",
       "4    3      0  1.0  0.468085       0.0  0.299540    0.493976  0.000000   \n",
       "5    3      1  1.0  0.446809       1.0  0.451776    0.060241  0.680000   \n",
       "6    4      0  1.0  0.404255       0.0  1.000000    0.003012  0.000000   \n",
       "7    4      1  1.0  0.531915       1.0  0.104733    0.060241  0.533333   \n",
       "8    5      1  1.0  0.468085       0.0  0.615205    0.060241  0.664000   \n",
       "9    5      0  1.0  0.468085       0.0  0.410244    0.012048  0.746667   \n",
       "10   6      0  1.0  0.382979       0.0  0.540006    0.000000  0.586667   \n",
       "11   6      1  1.0  0.468085       0.0  0.314584    0.060241  0.600000   \n",
       "12   7      0  1.0  0.531915       0.0  0.200924    0.349398  0.800000   \n",
       "13   7      1  1.0  0.446809       0.0  0.344528    0.048193  0.561333   \n",
       "14   8      1  1.0  0.468085       0.0  0.464895    0.060241  0.885333   \n",
       "15   8      0  1.0  0.425532       0.0  0.464895    0.060241  0.000000   \n",
       "16   9      0  0.0  0.702128       1.0  0.519299    0.132530  0.000000   \n",
       "17   9      1  0.0  0.638298       1.0  0.306468    0.012048  0.626667   \n",
       "18  10      1  1.0  0.744681       0.0  0.297872    0.000000  0.617333   \n",
       "19  10      0  1.0  0.638298       0.0  0.468085    0.000000  0.586667   \n",
       "20  11      0  1.0  0.319149       1.0  0.510638    0.060241  0.000000   \n",
       "21  11      1  1.0  0.276596       0.0  0.174109    0.060241  0.573333   \n",
       "22  12      0  1.0  0.659574       1.0  0.314584    0.012048  0.613333   \n",
       "23  12      1  1.0  0.723404       1.0  0.496123    0.018072  0.546667   \n",
       "24  13      0  0.0  0.723404       1.0  0.445676    0.277108  0.000000   \n",
       "25  13      1  0.0  0.787234       1.0  0.365373    0.042169  0.520000   \n",
       "26  14      0  1.0  0.553191       0.0  0.205838    0.042169  0.800000   \n",
       "27  14      1  1.0  0.404255       0.0  0.363675    0.060241  0.711467   \n",
       "28  15      0  1.0  0.659574       0.0  0.216840    0.060241  0.746667   \n",
       "29  15      1  1.0  0.617021       0.0  0.319625    0.277108  0.586667   \n",
       "..  ..    ...  ...       ...       ...       ...         ...       ...   \n",
       "52  27      0  1.0  0.510638       1.0  0.226950    0.277108  0.660000   \n",
       "53  27      1  1.0  0.489362       1.0  0.397163    0.060241  0.573333   \n",
       "54  28      0  1.0  0.276596       0.0  0.423552    0.060241  0.533333   \n",
       "55  28      1  1.0  0.234043       1.0  0.469070    0.003012  0.581333   \n",
       "56  29      0  0.0  0.617021       1.0  0.561390    0.277108  0.605333   \n",
       "57  29      1  0.0  0.744681       1.0  0.437679    0.027108  0.566667   \n",
       "58  30      0  1.0  0.531915       0.0  0.583928    0.349398  0.546667   \n",
       "59  30      1  1.0  0.510638       0.0  0.302314    0.021084  0.600000   \n",
       "60  31      1  1.0  0.510638       0.0  0.423552    0.024096  0.000000   \n",
       "61  31      0  1.0  0.425532       0.0  0.613181    0.060241  0.684000   \n",
       "62  32      0  1.0  0.319149       0.0  0.564890    0.030120  0.000000   \n",
       "63  32      1  1.0  0.191489       0.0  0.331815    0.060241  0.000000   \n",
       "64  33      0  0.0  0.531915       1.0  0.219760    0.204819  0.730667   \n",
       "65  33      1  0.0  0.468085       0.0  0.255546    0.033133  0.626667   \n",
       "66  34      0  1.0  0.851064       1.0  0.538877    0.349398  0.558667   \n",
       "67  34      1  1.0  0.851064       0.0  0.226950    0.132530  0.693333   \n",
       "68  35      1  1.0  0.574468       0.0  0.158106    0.204819  0.000000   \n",
       "69  35      0  1.0  0.659574       0.0  0.467095    0.132530  0.673333   \n",
       "70  36      1  1.0  0.468085       1.0  0.496123    0.060241  0.000000   \n",
       "71  36      0  1.0  0.510638       1.0  0.510510    0.710843  0.533333   \n",
       "72  37      1  1.0  0.553191       1.0  0.416741    0.060241  0.586667   \n",
       "73  37      0  1.0  0.489362       0.0  0.397163    0.060241  0.666667   \n",
       "74  38      1  1.0  0.319149       0.0  0.539973    0.012048  0.853333   \n",
       "75  38      0  1.0  0.276596       0.0  0.647606    0.042169  0.521333   \n",
       "76  39      0  1.0  0.617021       0.0  0.435507    0.493976  0.626667   \n",
       "77  39      1  1.0  0.638298       1.0  0.314584    0.132530  0.640000   \n",
       "78  40      1  0.0  0.574468       1.0  0.142233    0.060241  0.000000   \n",
       "79  40      0  0.0  0.446809       1.0  0.449864    0.060241  0.908000   \n",
       "80  41      1  0.0  0.638298       1.0  0.437621    0.024096  0.000000   \n",
       "81  41      0  0.0  0.765957       1.0  0.450216    0.132530  0.586667   \n",
       "\n",
       "    diabetes  hs_cad  hyperlipidemia  HBP       SBP       DBP  ECG  \\\n",
       "0        1.0     0.0             0.0  1.0  0.624060  0.855263  1.0   \n",
       "1        1.0     0.0             1.0  1.0  0.368421  0.697368  1.0   \n",
       "2        0.0     0.0             0.0  1.0  0.368421  0.697368  0.0   \n",
       "3        0.0     0.0             0.0  1.0  0.548872  0.723684  0.0   \n",
       "4        0.0     0.0             0.0  1.0  0.398496  0.592105  1.0   \n",
       "5        0.0     0.0             0.0  1.0  0.466165  0.263158  1.0   \n",
       "6        0.0     0.0             0.0  0.0  0.624060  0.723684  0.0   \n",
       "7        0.0     0.0             0.0  1.0  0.503759  0.539474  0.0   \n",
       "8        0.0     0.0             0.0  1.0  0.285714  0.328947  1.0   \n",
       "9        0.0     0.0             0.0  1.0  0.406015  0.342105  0.0   \n",
       "10       0.0     0.0             1.0  1.0  0.451128  0.328947  1.0   \n",
       "11       0.0     0.0             0.0  1.0  0.819549  0.907895  1.0   \n",
       "12       0.0     0.0             0.0  1.0  0.353383  0.421053  1.0   \n",
       "13       0.0     0.0             0.0  1.0  0.548872  0.460526  1.0   \n",
       "14       0.0     0.0             1.0  1.0  0.263158  0.592105  1.0   \n",
       "15       0.0     0.0             1.0  1.0  0.263158  0.592105  1.0   \n",
       "16       1.0     0.0             0.0  1.0  0.571429  0.934211  1.0   \n",
       "17       0.0     0.0             0.0  0.0  0.000000  0.342105  1.0   \n",
       "18       0.0     0.0             0.0  1.0  1.000000  1.000000  1.0   \n",
       "19       0.0     0.0             1.0  1.0  0.398496  0.460526  1.0   \n",
       "20       0.0     0.0             0.0  0.0  0.571429  0.710526  1.0   \n",
       "21       0.0     0.0             0.0  1.0  0.533835  0.592105  1.0   \n",
       "22       0.0     0.0             0.0  0.0  0.548872  0.723684  0.0   \n",
       "23       0.0     0.0             0.0  0.0  0.736842  0.802632  1.0   \n",
       "24       0.0     0.0             0.0  0.0  0.488722  0.710526  0.0   \n",
       "25       0.0     0.0             0.0  1.0  0.278196  0.723684  1.0   \n",
       "26       0.0     0.0             0.0  1.0  0.398496  0.723684  1.0   \n",
       "27       0.0     0.0             1.0  1.0  0.827068  0.934211  1.0   \n",
       "28       0.0     0.0             0.0  1.0  0.390977  0.592105  1.0   \n",
       "29       0.0     0.0             0.0  1.0  0.488722  0.605263  1.0   \n",
       "..       ...     ...             ...  ...       ...       ...  ...   \n",
       "52       0.0     0.0             0.0  0.0  0.240602  0.473684  1.0   \n",
       "53       0.0     0.0             0.0  1.0  0.556391  0.671053  1.0   \n",
       "54       0.0     0.0             0.0  1.0  0.308271  0.486842  1.0   \n",
       "55       0.0     0.0             0.0  1.0  0.127820  0.328947  1.0   \n",
       "56       0.0     0.0             1.0  1.0  0.774436  0.763158  0.0   \n",
       "57       0.0     0.0             0.0  1.0  0.225564  0.342105  0.0   \n",
       "58       0.0     0.0             1.0  1.0  0.548872  0.697368  1.0   \n",
       "59       0.0     0.0             0.0  1.0  0.436090  0.605263  0.0   \n",
       "60       0.0     0.0             0.0  1.0  0.624060  0.723684  1.0   \n",
       "61       0.0     0.0             1.0  1.0  0.684211  1.000000  1.0   \n",
       "62       0.0     0.0             0.0  1.0  0.661654  0.842105  1.0   \n",
       "63       0.0     0.0             0.0  1.0  0.421053  0.539474  1.0   \n",
       "64       0.0     0.0             0.0  1.0  0.240602  0.657895  1.0   \n",
       "65       0.0     0.0             0.0  1.0  0.360902  0.552632  1.0   \n",
       "66       0.0     0.0             0.0  1.0  0.548872  0.789474  1.0   \n",
       "67       0.0     0.0             0.0  1.0  0.413534  0.605263  1.0   \n",
       "68       0.0     0.0             0.0  1.0  0.451128  0.802632  0.0   \n",
       "69       0.0     0.0             0.0  1.0  0.729323  0.907895  0.0   \n",
       "70       0.0     0.0             1.0  1.0  0.624060  0.592105  1.0   \n",
       "71       0.0     0.0             1.0  1.0  0.345865  0.486842  1.0   \n",
       "72       0.0     0.0             0.0  0.0  0.458647  0.513158  0.0   \n",
       "73       0.0     0.0             0.0  0.0  0.323308  0.592105  1.0   \n",
       "74       0.0     0.0             0.0  1.0  0.323308  0.592105  1.0   \n",
       "75       0.0     1.0             0.0  1.0  0.759399  0.000000  1.0   \n",
       "76       0.0     0.0             0.0  1.0  0.458647  0.592105  1.0   \n",
       "77       0.0     0.0             0.0  0.0  0.436090  0.671053  1.0   \n",
       "78       0.0     0.0             0.0  1.0  0.473684  0.657895  0.0   \n",
       "79       0.0     1.0             0.0  1.0  0.571429  0.855263  1.0   \n",
       "80       0.0     0.0             0.0  0.0  0.473684  0.723684  1.0   \n",
       "81       1.0     0.0             0.0  1.0  0.488722  0.776316  1.0   \n",
       "\n",
       "    kidney_failure  ln_ddimer  limb_ischemia  tamponade_presurgbin  \n",
       "0              0.0   0.847246            0.0                   0.0  \n",
       "1              0.0   0.426389            0.0                   1.0  \n",
       "2              0.0   0.669479            0.0                   0.0  \n",
       "3              0.0   0.421741            0.0                   1.0  \n",
       "4              0.0   1.000000            0.0                   0.0  \n",
       "5              0.0   0.651064            1.0                   0.0  \n",
       "6              0.0   0.805514            0.0                   0.0  \n",
       "7              0.0   0.646203            0.0                   0.0  \n",
       "8              0.0   0.997573            0.0                   1.0  \n",
       "9              0.0   1.000000            1.0                   1.0  \n",
       "10             0.0   0.487064            0.0                   0.0  \n",
       "11             0.0   0.265573            0.0                   0.0  \n",
       "12             0.0   0.276569            0.0                   0.0  \n",
       "13             0.0   0.401288            0.0                   0.0  \n",
       "14             0.0   0.454797            0.0                   0.0  \n",
       "15             0.0   0.444577            0.0                   0.0  \n",
       "16             0.0   0.391629            0.0                   1.0  \n",
       "17             0.0   0.412432            0.0                   1.0  \n",
       "18             0.0   0.000000            0.0                   0.0  \n",
       "19             0.0   0.071753            0.0                   0.0  \n",
       "20             0.0   0.264851            0.0                   0.0  \n",
       "21             0.0   0.261745            0.0                   0.0  \n",
       "22             0.0   0.890379            0.0                   0.0  \n",
       "23             0.0   0.461134            0.0                   1.0  \n",
       "24             0.0   0.219296            0.0                   1.0  \n",
       "25             0.0   0.457584            0.0                   0.0  \n",
       "26             0.0   0.315563            0.0                   0.0  \n",
       "27             0.0   0.399646            0.0                   0.0  \n",
       "28             0.0   0.424188            0.0                   0.0  \n",
       "29             0.0   0.469014            0.0                   0.0  \n",
       "..             ...        ...            ...                   ...  \n",
       "52             0.0   0.243274            0.0                   1.0  \n",
       "53             0.0   0.950695            1.0                   1.0  \n",
       "54             0.0   1.000000            0.0                   0.0  \n",
       "55             0.0   0.469947            0.0                   1.0  \n",
       "56             0.0   0.257642            0.0                   0.0  \n",
       "57             0.0   0.436856            0.0                   0.0  \n",
       "58             0.0   0.419112            0.0                   0.0  \n",
       "59             0.0   0.695326            0.0                   0.0  \n",
       "60             0.0   0.435389            0.0                   0.0  \n",
       "61             0.0   0.275552            1.0                   0.0  \n",
       "62             1.0   0.360961            0.0                   0.0  \n",
       "63             0.0   0.914786            1.0                   0.0  \n",
       "64             0.0   0.144028            0.0                   0.0  \n",
       "65             0.0   0.426600            0.0                   0.0  \n",
       "66             0.0   0.282549            0.0                   0.0  \n",
       "67             0.0   0.600323            0.0                   0.0  \n",
       "68             0.0   0.226463            0.0                   0.0  \n",
       "69             0.0   0.229827            0.0                   0.0  \n",
       "70             0.0   0.410121            0.0                   0.0  \n",
       "71             0.0   0.418743            1.0                   0.0  \n",
       "72             0.0   0.632627            0.0                   0.0  \n",
       "73             0.0   0.402913            0.0                   0.0  \n",
       "74             0.0   0.204766            0.0                   0.0  \n",
       "75             0.0   0.406119            0.0                   0.0  \n",
       "76             0.0   0.138065            0.0                   0.0  \n",
       "77             0.0   0.186619            1.0                   0.0  \n",
       "78             0.0   0.193343            0.0                   0.0  \n",
       "79             0.0   0.096656            0.0                   0.0  \n",
       "80             0.0   0.274014            0.0                   0.0  \n",
       "81             0.0   0.479316            0.0                   0.0  \n",
       "\n",
       "[82 rows x 19 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=handle_case_attributes(csv_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21.2\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20_control',\n",
       " '20_event',\n",
       " '18_control',\n",
       " '18_event',\n",
       " '9_event',\n",
       " '9_control',\n",
       " '11_event',\n",
       " '11_control',\n",
       " '7_control',\n",
       " '7_event',\n",
       " '16_event',\n",
       " '16_control',\n",
       " '6_control',\n",
       " '6_event',\n",
       " '17_control',\n",
       " '17_event',\n",
       " '1_event',\n",
       " '1_control',\n",
       " '10_event',\n",
       " '10_control',\n",
       " '19_control',\n",
       " '19_event',\n",
       " '8_event',\n",
       " '8_control',\n",
       " '21_control',\n",
       " '21_event',\n",
       " '24_event',\n",
       " '24_control',\n",
       " '23_control',\n",
       " '23_event',\n",
       " '4_control',\n",
       " '4_event',\n",
       " '15_event',\n",
       " '15_control',\n",
       " '3_control',\n",
       " '3_event',\n",
       " '12_event',\n",
       " '12_control',\n",
       " '2_control',\n",
       " '2_event',\n",
       " '13_event',\n",
       " '13_control',\n",
       " '5_control',\n",
       " '5_event',\n",
       " '14_event',\n",
       " '14_control',\n",
       " '22_control',\n",
       " '25_control',\n",
       " '25_event']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.case_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainid_set={i.split(\"_\")[0] for i in dataset_train.case_id+dataset_val.case_id}\n",
    "testid_set={i.split(\"_\")[0] for i in dataset_test.case_id}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['id'].isin(trainid_set)]\n",
    "test_df = df[df['id'].isin(testid_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sex       age  smallbin       bmi  time_onset  ane_diam  diabetes  hs_cad  \\\n",
      "0   0.0  0.914894       1.0  0.246592    0.009036  0.666667       1.0     0.0   \n",
      "1   0.0  0.872340       1.0  0.459098    0.132530  0.752000       1.0     0.0   \n",
      "2   1.0  0.106383       0.0  0.364911    0.024096  0.626667       0.0     0.0   \n",
      "3   1.0  0.000000       0.0  0.525194    0.018072  0.000000       0.0     0.0   \n",
      "4   1.0  0.468085       0.0  0.299540    0.493976  0.000000       0.0     0.0   \n",
      "5   1.0  0.446809       1.0  0.451776    0.060241  0.680000       0.0     0.0   \n",
      "6   1.0  0.404255       0.0  1.000000    0.003012  0.000000       0.0     0.0   \n",
      "7   1.0  0.531915       1.0  0.104733    0.060241  0.533333       0.0     0.0   \n",
      "8   1.0  0.468085       0.0  0.615205    0.060241  0.664000       0.0     0.0   \n",
      "9   1.0  0.468085       0.0  0.410244    0.012048  0.746667       0.0     0.0   \n",
      "10  1.0  0.382979       0.0  0.540006    0.000000  0.586667       0.0     0.0   \n",
      "11  1.0  0.468085       0.0  0.314584    0.060241  0.600000       0.0     0.0   \n",
      "12  1.0  0.531915       0.0  0.200924    0.349398  0.800000       0.0     0.0   \n",
      "13  1.0  0.446809       0.0  0.344528    0.048193  0.561333       0.0     0.0   \n",
      "14  1.0  0.468085       0.0  0.464895    0.060241  0.885333       0.0     0.0   \n",
      "15  1.0  0.425532       0.0  0.464895    0.060241  0.000000       0.0     0.0   \n",
      "16  0.0  0.702128       1.0  0.519299    0.132530  0.000000       1.0     0.0   \n",
      "17  0.0  0.638298       1.0  0.306468    0.012048  0.626667       0.0     0.0   \n",
      "18  1.0  0.744681       0.0  0.297872    0.000000  0.617333       0.0     0.0   \n",
      "19  1.0  0.638298       0.0  0.468085    0.000000  0.586667       0.0     0.0   \n",
      "20  1.0  0.319149       1.0  0.510638    0.060241  0.000000       0.0     0.0   \n",
      "21  1.0  0.276596       0.0  0.174109    0.060241  0.573333       0.0     0.0   \n",
      "22  1.0  0.659574       1.0  0.314584    0.012048  0.613333       0.0     0.0   \n",
      "23  1.0  0.723404       1.0  0.496123    0.018072  0.546667       0.0     0.0   \n",
      "24  0.0  0.723404       1.0  0.445676    0.277108  0.000000       0.0     0.0   \n",
      "25  0.0  0.787234       1.0  0.365373    0.042169  0.520000       0.0     0.0   \n",
      "26  1.0  0.553191       0.0  0.205838    0.042169  0.800000       0.0     0.0   \n",
      "27  1.0  0.404255       0.0  0.363675    0.060241  0.711467       0.0     0.0   \n",
      "28  1.0  0.659574       0.0  0.216840    0.060241  0.746667       0.0     0.0   \n",
      "29  1.0  0.617021       0.0  0.319625    0.277108  0.586667       0.0     0.0   \n",
      "..  ...       ...       ...       ...         ...       ...       ...     ...   \n",
      "38  1.0  0.361702       1.0  0.536107    1.000000  0.000000       0.0     0.0   \n",
      "39  1.0  0.340426       0.0  0.363675    0.060241  0.000000       0.0     0.0   \n",
      "40  1.0  0.595745       0.0  0.547219    0.009036  0.000000       0.0     0.0   \n",
      "41  1.0  0.723404       1.0  0.299540    0.060241  0.901333       0.0     0.0   \n",
      "42  0.0  0.468085       0.0  0.437679    0.006024  0.000000       0.0     0.0   \n",
      "43  0.0  0.489362       1.0  0.376593    0.132530  0.600000       0.0     0.0   \n",
      "44  1.0  0.170213       0.0  0.765516    0.204819  0.556000       0.0     0.0   \n",
      "45  1.0  0.340426       0.0  0.379319    0.060241  0.000000       0.0     0.0   \n",
      "46  1.0  0.723404       0.0  0.427787    0.009036  0.654667       0.0     0.0   \n",
      "47  1.0  0.659574       1.0  0.510510    0.042169  0.000000       0.0     0.0   \n",
      "48  1.0  0.638298       0.0  0.314584    0.204819  0.717333       1.0     1.0   \n",
      "49  1.0  0.702128       0.0  0.290258    0.132530  0.000000       0.0     0.0   \n",
      "50  1.0  1.000000       1.0  0.536107    0.204819  0.560000       0.0     0.0   \n",
      "51  1.0  0.829787       0.0  0.326241    0.132530  0.000000       0.0     0.0   \n",
      "52  1.0  0.510638       1.0  0.226950    0.277108  0.660000       0.0     0.0   \n",
      "53  1.0  0.489362       1.0  0.397163    0.060241  0.573333       0.0     0.0   \n",
      "54  1.0  0.276596       0.0  0.423552    0.060241  0.533333       0.0     0.0   \n",
      "55  1.0  0.234043       1.0  0.469070    0.003012  0.581333       0.0     0.0   \n",
      "56  0.0  0.617021       1.0  0.561390    0.277108  0.605333       0.0     0.0   \n",
      "57  0.0  0.744681       1.0  0.437679    0.027108  0.566667       0.0     0.0   \n",
      "58  1.0  0.531915       0.0  0.583928    0.349398  0.546667       0.0     0.0   \n",
      "59  1.0  0.510638       0.0  0.302314    0.021084  0.600000       0.0     0.0   \n",
      "60  1.0  0.510638       0.0  0.423552    0.024096  0.000000       0.0     0.0   \n",
      "61  1.0  0.425532       0.0  0.613181    0.060241  0.684000       0.0     0.0   \n",
      "62  1.0  0.319149       0.0  0.564890    0.030120  0.000000       0.0     0.0   \n",
      "63  1.0  0.191489       0.0  0.331815    0.060241  0.000000       0.0     0.0   \n",
      "64  0.0  0.531915       1.0  0.219760    0.204819  0.730667       0.0     0.0   \n",
      "65  0.0  0.468085       0.0  0.255546    0.033133  0.626667       0.0     0.0   \n",
      "66  1.0  0.851064       1.0  0.538877    0.349398  0.558667       0.0     0.0   \n",
      "67  1.0  0.851064       0.0  0.226950    0.132530  0.693333       0.0     0.0   \n",
      "\n",
      "    hyperlipidemia  HBP       SBP       DBP  ECG  kidney_failure  ln_ddimer  \\\n",
      "0              0.0  1.0  0.624060  0.855263  1.0             0.0   0.847246   \n",
      "1              1.0  1.0  0.368421  0.697368  1.0             0.0   0.426389   \n",
      "2              0.0  1.0  0.368421  0.697368  0.0             0.0   0.669479   \n",
      "3              0.0  1.0  0.548872  0.723684  0.0             0.0   0.421741   \n",
      "4              0.0  1.0  0.398496  0.592105  1.0             0.0   1.000000   \n",
      "5              0.0  1.0  0.466165  0.263158  1.0             0.0   0.651064   \n",
      "6              0.0  0.0  0.624060  0.723684  0.0             0.0   0.805514   \n",
      "7              0.0  1.0  0.503759  0.539474  0.0             0.0   0.646203   \n",
      "8              0.0  1.0  0.285714  0.328947  1.0             0.0   0.997573   \n",
      "9              0.0  1.0  0.406015  0.342105  0.0             0.0   1.000000   \n",
      "10             1.0  1.0  0.451128  0.328947  1.0             0.0   0.487064   \n",
      "11             0.0  1.0  0.819549  0.907895  1.0             0.0   0.265573   \n",
      "12             0.0  1.0  0.353383  0.421053  1.0             0.0   0.276569   \n",
      "13             0.0  1.0  0.548872  0.460526  1.0             0.0   0.401288   \n",
      "14             1.0  1.0  0.263158  0.592105  1.0             0.0   0.454797   \n",
      "15             1.0  1.0  0.263158  0.592105  1.0             0.0   0.444577   \n",
      "16             0.0  1.0  0.571429  0.934211  1.0             0.0   0.391629   \n",
      "17             0.0  0.0  0.000000  0.342105  1.0             0.0   0.412432   \n",
      "18             0.0  1.0  1.000000  1.000000  1.0             0.0   0.000000   \n",
      "19             1.0  1.0  0.398496  0.460526  1.0             0.0   0.071753   \n",
      "20             0.0  0.0  0.571429  0.710526  1.0             0.0   0.264851   \n",
      "21             0.0  1.0  0.533835  0.592105  1.0             0.0   0.261745   \n",
      "22             0.0  0.0  0.548872  0.723684  0.0             0.0   0.890379   \n",
      "23             0.0  0.0  0.736842  0.802632  1.0             0.0   0.461134   \n",
      "24             0.0  0.0  0.488722  0.710526  0.0             0.0   0.219296   \n",
      "25             0.0  1.0  0.278196  0.723684  1.0             0.0   0.457584   \n",
      "26             0.0  1.0  0.398496  0.723684  1.0             0.0   0.315563   \n",
      "27             1.0  1.0  0.827068  0.934211  1.0             0.0   0.399646   \n",
      "28             0.0  1.0  0.390977  0.592105  1.0             0.0   0.424188   \n",
      "29             0.0  1.0  0.488722  0.605263  1.0             0.0   0.469014   \n",
      "..             ...  ...       ...       ...  ...             ...        ...   \n",
      "38             0.0  0.0  0.436090  0.526316  1.0             0.0   0.424545   \n",
      "39             0.0  1.0  0.285714  0.381579  1.0             0.0   0.431736   \n",
      "40             0.0  1.0  0.218045  0.223684  1.0             0.0   1.000000   \n",
      "41             0.0  0.0  0.473684  0.657895  1.0             0.0   0.681883   \n",
      "42             1.0  1.0  0.413534  0.263158  1.0             0.0   0.680527   \n",
      "43             0.0  0.0  0.187970  0.486842  0.0             0.0   0.456522   \n",
      "44             1.0  1.0  0.654135  0.723684  1.0             0.0   0.419406   \n",
      "45             1.0  0.0  0.255639  0.342105  1.0             0.0   0.351291   \n",
      "46             0.0  1.0  0.503759  0.789474  1.0             0.0   0.895661   \n",
      "47             0.0  1.0  0.270677  0.368421  1.0             1.0   0.706695   \n",
      "48             0.0  1.0  0.533835  0.723684  1.0             0.0   0.111451   \n",
      "49             0.0  1.0  0.360902  0.947368  0.0             0.0   0.243684   \n",
      "50             1.0  1.0  0.398496  0.592105  1.0             0.0   0.037912   \n",
      "51             0.0  0.0  0.458647  0.250000  1.0             0.0   0.484419   \n",
      "52             0.0  0.0  0.240602  0.473684  1.0             0.0   0.243274   \n",
      "53             0.0  1.0  0.556391  0.671053  1.0             0.0   0.950695   \n",
      "54             0.0  1.0  0.308271  0.486842  1.0             0.0   1.000000   \n",
      "55             0.0  1.0  0.127820  0.328947  1.0             0.0   0.469947   \n",
      "56             1.0  1.0  0.774436  0.763158  0.0             0.0   0.257642   \n",
      "57             0.0  1.0  0.225564  0.342105  0.0             0.0   0.436856   \n",
      "58             1.0  1.0  0.548872  0.697368  1.0             0.0   0.419112   \n",
      "59             0.0  1.0  0.436090  0.605263  0.0             0.0   0.695326   \n",
      "60             0.0  1.0  0.624060  0.723684  1.0             0.0   0.435389   \n",
      "61             1.0  1.0  0.684211  1.000000  1.0             0.0   0.275552   \n",
      "62             0.0  1.0  0.661654  0.842105  1.0             1.0   0.360961   \n",
      "63             0.0  1.0  0.421053  0.539474  1.0             0.0   0.914786   \n",
      "64             0.0  1.0  0.240602  0.657895  1.0             0.0   0.144028   \n",
      "65             0.0  1.0  0.360902  0.552632  1.0             0.0   0.426600   \n",
      "66             0.0  1.0  0.548872  0.789474  1.0             0.0   0.282549   \n",
      "67             0.0  1.0  0.413534  0.605263  1.0             0.0   0.600323   \n",
      "\n",
      "    limb_ischemia  tamponade_presurgbin  \n",
      "0             0.0                   0.0  \n",
      "1             0.0                   1.0  \n",
      "2             0.0                   0.0  \n",
      "3             0.0                   1.0  \n",
      "4             0.0                   0.0  \n",
      "5             1.0                   0.0  \n",
      "6             0.0                   0.0  \n",
      "7             0.0                   0.0  \n",
      "8             0.0                   1.0  \n",
      "9             1.0                   1.0  \n",
      "10            0.0                   0.0  \n",
      "11            0.0                   0.0  \n",
      "12            0.0                   0.0  \n",
      "13            0.0                   0.0  \n",
      "14            0.0                   0.0  \n",
      "15            0.0                   0.0  \n",
      "16            0.0                   1.0  \n",
      "17            0.0                   1.0  \n",
      "18            0.0                   0.0  \n",
      "19            0.0                   0.0  \n",
      "20            0.0                   0.0  \n",
      "21            0.0                   0.0  \n",
      "22            0.0                   0.0  \n",
      "23            0.0                   1.0  \n",
      "24            0.0                   1.0  \n",
      "25            0.0                   0.0  \n",
      "26            0.0                   0.0  \n",
      "27            0.0                   0.0  \n",
      "28            0.0                   0.0  \n",
      "29            0.0                   0.0  \n",
      "..            ...                   ...  \n",
      "38            0.0                   0.0  \n",
      "39            0.0                   0.0  \n",
      "40            0.0                   0.0  \n",
      "41            0.0                   0.0  \n",
      "42            1.0                   0.0  \n",
      "43            0.0                   1.0  \n",
      "44            0.0                   0.0  \n",
      "45            0.0                   0.0  \n",
      "46            0.0                   0.0  \n",
      "47            1.0                   0.0  \n",
      "48            0.0                   0.0  \n",
      "49            0.0                   0.0  \n",
      "50            0.0                   0.0  \n",
      "51            0.0                   0.0  \n",
      "52            0.0                   1.0  \n",
      "53            1.0                   1.0  \n",
      "54            0.0                   0.0  \n",
      "55            0.0                   1.0  \n",
      "56            0.0                   0.0  \n",
      "57            0.0                   0.0  \n",
      "58            0.0                   0.0  \n",
      "59            0.0                   0.0  \n",
      "60            0.0                   0.0  \n",
      "61            1.0                   0.0  \n",
      "62            0.0                   0.0  \n",
      "63            1.0                   0.0  \n",
      "64            0.0                   0.0  \n",
      "65            0.0                   0.0  \n",
      "66            0.0                   0.0  \n",
      "67            0.0                   0.0  \n",
      "\n",
      "[68 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "X = train_df[train_df.columns[2:]]\n",
    "Y = train_df[\"group\"]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_df[test_df.columns[2:]]\n",
    "test_y = test_df[\"group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k_range = range(1,31)\n",
    "cv_scores = []  #用来放每个模型的结果值\n",
    "for n in k_range:\n",
    "    knn = KNeighborsClassifier(n)   #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV\n",
    "    scores = cross_val_score(knn,X,Y,cv=10,scoring='accuracy')  #cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。\n",
    "    cv_scores.append(scores.mean())\n",
    "plt.plot(k_range,cv_scores)\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')  #通过图像选择最好的参数\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21428571428571427\n"
     ]
    }
   ],
   "source": [
    "best_knn = KNeighborsClassifier(n_neighbors=10)\t# 选择最优的K=3传入模型\n",
    "best_knn.fit(X,Y)\t\t\t#训练模型\n",
    "print(best_knn.score(test_x,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/discriminant_analysis.py:693: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        glm      tree       svm  RandomForest       knn       lda       qda  \\\n",
      "0  1.000000  0.833333  1.000000      0.875000  1.000000  1.000000  0.875000   \n",
      "1  0.666667  0.625000  0.833333      0.833333  0.833333  0.666667  0.833333   \n",
      "2  0.666667  0.625000  0.666667      0.833333  0.833333  0.666667  0.666667   \n",
      "3  0.625000  0.500000  0.500000      0.750000  0.625000  0.625000  0.500000   \n",
      "4  0.625000  0.500000  0.500000      0.750000  0.500000  0.625000  0.500000   \n",
      "5  0.625000  0.500000  0.500000      0.666667  0.500000  0.500000  0.500000   \n",
      "6  0.500000  0.500000  0.500000      0.500000  0.375000  0.500000  0.500000   \n",
      "7  0.500000  0.375000  0.500000      0.500000  0.375000  0.500000  0.375000   \n",
      "8  0.375000  0.333333  0.500000      0.375000  0.333333  0.500000  0.333333   \n",
      "9  0.333333  0.166667  0.333333      0.333333  0.333333  0.333333  0.166667   \n",
      "\n",
      "        gnb       mbp       ada  \n",
      "0  0.666667  0.750000  0.833333  \n",
      "1  0.666667  0.666667  0.750000  \n",
      "2  0.500000  0.625000  0.666667  \n",
      "3  0.500000  0.500000  0.666667  \n",
      "4  0.500000  0.500000  0.666667  \n",
      "5  0.500000  0.500000  0.500000  \n",
      "6  0.500000  0.500000  0.375000  \n",
      "7  0.500000  0.375000  0.375000  \n",
      "8  0.333333  0.250000  0.375000  \n",
      "9  0.333333  0.166667  0.333333  \n",
      "按照accuracy进行排序\n",
      "RandomForest    0.641667\n",
      "lda             0.591667\n",
      "glm             0.591667\n",
      "svm             0.583333\n",
      "knn             0.570833\n",
      "ada             0.554167\n",
      "qda             0.525000\n",
      "gnb             0.500000\n",
      "tree            0.495833\n",
      "mbp             0.483333\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "metric_all = pd.DataFrame()\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n",
    "lr = LogisticRegression(C=1000, solver='liblinear', random_state=0)\n",
    "metric = cross_val_score(lr, X, Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['glm'] = metric[::-1]\n",
    " \n",
    "# 拟合决策树模型\n",
    "from sklearn import tree\n",
    " \n",
    "tree = tree.DecisionTreeClassifier(criterion='gini')\n",
    "metric = cross_val_score(tree, X, Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['tree'] = metric[::-1]\n",
    " \n",
    "# 拟合svm模型\n",
    "from sklearn import svm\n",
    " \n",
    "svc = svm.SVC(C=1.0, kernel='rbf', gamma='auto')\n",
    "metric = cross_val_score(svc, X, Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['svm'] = metric[::-1]\n",
    " \n",
    "# 拟合随机森林算法\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    " \n",
    "RF = RandomForestClassifier(n_estimators=30, criterion='gini', random_state=10)\n",
    "metric = cross_val_score(RF, X, Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['RandomForest'] = metric[::-1]\n",
    " \n",
    "# 构造knn最近邻模型\n",
    "from sklearn import neighbors\n",
    " \n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=10, algorithm='kd_tree')\n",
    "metric = cross_val_score(estimator=knn, X=X, y=Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['knn'] = metric[::-1]\n",
    " \n",
    "# 构造lda模型\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    " \n",
    "lda = LinearDiscriminantAnalysis(solver='lsqr', shrinkage=None, priors=None)\n",
    "metric = cross_val_score(estimator=lda, X=X, y=Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['lda'] = metric[::-1]\n",
    " \n",
    "# 构造qda模型\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    " \n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "metric = cross_val_score(estimator=qda, X=X, y=Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['qda'] = metric[::-1]\n",
    " \n",
    "#  高斯朴素贝叶斯,多项式贝叶斯适用于文本分类，伯努利贝叶斯需要全部变量威二值变量\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    " \n",
    "gnb = GaussianNB()\n",
    "metric = cross_val_score(estimator=gnb, X=X, y=Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['gnb'] = metric[::-1]\n",
    " \n",
    "# sklearn提供了bp多层神经网络，隐含层设为 (4, 3, 2)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    " \n",
    "mbp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(4, 3, 2), random_state=1)\n",
    "metric = cross_val_score(estimator=mbp, X=X, y=Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['mbp'] = metric[::-1]\n",
    " \n",
    "# 采用adaboost算法\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    " \n",
    "# 弱分类器的参数 base_estimator 默认为决策树\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "metric = cross_val_score(estimator=ada, X=X, y=Y, cv=10, scoring='accuracy')\n",
    "metric.sort()\n",
    "metric_all['ada'] = metric[::-1]\n",
    " \n",
    "# 将10种模型结果进行比较\n",
    "print(metric_all)\n",
    "print(\"按照accuracy进行排序\")\n",
    "metric_mean = metric_all.mean()\n",
    "print(metric_mean.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
